{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gDIVYixBGFsZ",
        "s8aBx8j9GIJt",
        "80M3vzBnGMDT"
      ],
      "authorship_tag": "ABX9TyO7g7Ul2hFQJBqV7L++XcTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mysertkaya/AI-Generated-Text-Detection/blob/main/Other_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability Model"
      ],
      "metadata": {
        "id": "gDIVYixBGFsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
      ],
      "metadata": {
        "id": "279qlvoSFO_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "BLFRQYjQFR8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring Algorithm"
      ],
      "metadata": {
        "id": "s8aBx8j9GIJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S70hSGNdFAij"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_rank_and_probability(model, tokenizer, text):\n",
        "    final_results = {}\n",
        "    final_results_raw = {}\n",
        "    print(\"Text length:\", len(text.split()))\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            inputs['input_ids'],\n",
        "            max_new_tokens=1,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=1,\n",
        "            do_sample=True,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            output_logits=True,\n",
        "        )\n",
        "\n",
        "    # List of scores (logits) from the model's output\n",
        "    all_logits = outputs.logits[0]  # List of tensors (one per generation step)\n",
        "\n",
        "    inputs_len = len(inputs['input_ids'][0])\n",
        "    for i in range(inputs_len-1):\n",
        "      logits = all_logits[i]\n",
        "\n",
        "      # Apply softmax to get probabilities from logits (raw probabilities)\n",
        "      probabilities = F.softmax(logits, dim=0)\n",
        "      raw_scores = logits\n",
        "\n",
        "      # Get the last token from the next batch input (generated token)\n",
        "      last_token_id = inputs['input_ids'][0][i+1]\n",
        "\n",
        "      # Find the probability of the last token\n",
        "      last_token_probability = probabilities[last_token_id].item()\n",
        "      last_token_probability_raw = raw_scores[last_token_id].item()\n",
        "      print(\"p:\", last_token_probability)\n",
        "      print(\"pr:\", last_token_probability_raw)\n",
        "\n",
        "      # Sort the probabilities in descending order and get top 20\n",
        "      sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
        "      sorted_probs_raw, sorted_indices_raw = torch.sort(raw_scores, descending=True)\n",
        "\n",
        "      # Get the rank of the last token in the sorted list\n",
        "      token_rank = (sorted_indices == last_token_id).nonzero(as_tuple=True)[0].item() + 1\n",
        "      token_rank_raw = (sorted_indices_raw == last_token_id).nonzero(as_tuple=True)[0].item() + 1\n",
        "\n",
        "      # Decode the last token to actual token (text)\n",
        "      last_token = tokenizer.decode([last_token_id])\n",
        "\n",
        "      # Print the rank and raw probability of the last token\n",
        "      final_results[last_token] = (token_rank, last_token_probability)\n",
        "      final_results_raw[last_token] = (token_rank_raw, last_token_probability_raw)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    from collections import Counter\n",
        "\n",
        "    ranks = [token_rank for token_rank, _ in final_results.values()]\n",
        "\n",
        "    rank_frequency = Counter(ranks)\n",
        "\n",
        "    sorted_ranks = sorted(rank_frequency.items())\n",
        "\n",
        "    # # Separate ranks and their frequencies for plotting\n",
        "    # x = [rank for rank, freq in sorted_ranks]\n",
        "    # y = [freq for rank, freq in sorted_ranks]\n",
        "\n",
        "    # # Plot the graph\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "    # plt.bar(x, y, color='skyblue', edgecolor='black')\n",
        "    # plt.title('Frequency Distribution of Ranks')\n",
        "    # plt.xlabel('Rank')\n",
        "    # plt.ylabel('Frequency')\n",
        "    # plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    # plt.show()\n",
        "\n",
        "    # Initialize counters\n",
        "    total_count = 0\n",
        "    count_below_50 = 0\n",
        "    count_below_10 = 0\n",
        "    count_below_20 = 0\n",
        "    count_below_20000 = 0\n",
        "\n",
        "    token_probabilities = 0\n",
        "    token_probabilities_raw = 0\n",
        "    token_probabilities_raw_combined = 0\n",
        "\n",
        "    # Iterate through the dictionary and count ranks based on conditions\n",
        "    for token, (token_rank, token_probability) in final_results.items():\n",
        "        total_count += 1\n",
        "        if token_rank < 10:\n",
        "            count_below_10 += 1\n",
        "        if token_rank < 20:\n",
        "            count_below_20 += 1\n",
        "        if token_rank < 50:\n",
        "            count_below_50 += 1\n",
        "        if token_rank < 20000:\n",
        "            count_below_20000 += 1\n",
        "\n",
        "        if token_probability == 0:\n",
        "          variance = 0.1\n",
        "          token_probabilities += (1/token_rank) * variance\n",
        "        else:\n",
        "          token_probabilities += (1/token_rank) * token_probability\n",
        "\n",
        "    for token, (token_rank, token_probability) in final_results_raw.items():\n",
        "        if token_probability == 0:\n",
        "          variance = 0.1\n",
        "          token_probabilities_raw_combined += (1/token_rank) * variance\n",
        "        else:\n",
        "          token_probabilities_raw_combined += (1/token_rank) * token_probability\n",
        "        token_probabilities_raw += token_probability\n",
        "    # Store the counts\n",
        "    rank_counts = {\n",
        "        'total_count': total_count,\n",
        "        'count_below_10': count_below_10,\n",
        "        'count_below_20': count_below_20,\n",
        "        'count_below_50': count_below_50,\n",
        "        'count_below_20000': count_below_20000\n",
        "    }\n",
        "    print(rank_counts)\n",
        "    print(\"Softmax:\")\n",
        "    print(\"Tp/tc:\", token_probabilities / len(final_results))\n",
        "    print(\"10/tc:\", rank_counts['count_below_10'] / rank_counts['total_count'])\n",
        "    print(\"20/tc:\", rank_counts['count_below_20'] / rank_counts['total_count'])\n",
        "    print(\"50/tc:\", rank_counts['count_below_50'] / rank_counts['total_count'])\n",
        "    print(\"20000/tc:\", rank_counts['count_below_20000'] / rank_counts['total_count'])\n",
        "    print(\"Raw:\")\n",
        "    print(\"Tp/tc:\", token_probabilities_raw / len(final_results_raw))\n",
        "    print(\"Tpc/tc:\", token_probabilities_raw_combined / len(final_results_raw))\n",
        "\n",
        "    return rank_counts, token_probabilities, final_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "80M3vzBnGMDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_text=\"\"\"The waves crash vigorously against jagged rocks jutting out from the shore, sending plumes of frothy white spray high into the air. The pale blue water roils and churns, distorting the golden light of the setting sun reflecting off the waves. Each time the waves hit the rocks, they burst forth with a thunderous boom that fills the air, accompanied by the cacophony of screeching seagulls swooping down in search of any exposed creatures. The vast horizon stretches out endlessly before me, the line where pale blue sky meets dark blue sea blurred and indistinct. Patches of burnt orange and gold clouds line the horizon, remnants of the day's light dissolving into the depths of the ocean. In the distance, a lone sailboat navigates the waters, its white triangular sail swaying gently in the offshore breeze. The warm late afternoon air carries the unmistakable tang of salt water and kelp, invading my senses and transporting me fully into this seaside escape. I close my eyes and breathe deeply, feeling the sea mist on my face and listening to the rhythmic crash of wave after wave. When I open my eyes again, the sunset has painted the scene in hues of peach and rose-gold, giving the rocks, water and distant sailboat an almost ethereal glow. I turn away reluctantly, knowing my visit to this magical place must end for now but carrying with me its vivid sights, sounds and smells etched permanently into my mind.\"\"\""
      ],
      "metadata": {
        "id": "UjXdwKkIGBek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_results = calculate_rank_and_probability(model, tokenizer, text)"
      ],
      "metadata": {
        "id": "dMo1ftGhGU0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}